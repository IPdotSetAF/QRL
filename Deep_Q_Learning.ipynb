{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "Here I try to implement the Reinforcement Learning technique called Deep Q-learning (or Deep Q-Network, hence DQN) which uses a Neural Network (in our case will be a Parametrized Quantum Circuit) to approximate the Q-values.\n",
    "\n",
    "This is the same technique used in the paper by Skolik. \n",
    "\n",
    "I will mostly copy the code from this great book *Hands on Machine Learning*, which has accompanying free tutorials here:  \n",
    "https://github.com/ageron/handson-ml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Operator Imports\n",
    "from qiskit.aqua.operators import Z, I, StateFn, CircuitStateFn, SummedOp\n",
    "from qiskit.aqua.operators.gradients import Gradient, NaturalGradient, QFI, Hessian\n",
    "\n",
    "#Circuit imports\n",
    "from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter, ParameterVector, ParameterExpression\n",
    "\n",
    "#Qiskit imports\n",
    "import qiskit as qk\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "import qiskit_machine_learning as qkml\n",
    "from qiskit.utils import QuantumInstance\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch \n",
    "import gym \n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_circuit(inputs, encoding_weights, num_qubits = 4, gate = 'rx', *args):\n",
    "    \"\"\"\n",
    "    Encode classical input data on a quantum circuit. \n",
    "    \n",
    "    To be used inside the `parametrized_circuit` function. \n",
    "    \n",
    "    Args\n",
    "    -------\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    TODO:\n",
    "    1. Add choice of encoding gate: ry, rz, or rx.\n",
    "    \"\"\"\n",
    "    \n",
    "    qc = qk.QuantumCircuit(num_qubits)\n",
    "    \n",
    "    # Encode data with a RX rotation\n",
    "    for i, data in enumerate(inputs): \n",
    "        qc.rx(encoding_weights[i]*inputs[i], i)\n",
    "        \n",
    "    return qc\n",
    "\n",
    "def parametrized_circuit(num_qubits = 4, reuploading = False, reps = 2, insert_barriers = True, meas = False):\n",
    "    \"\"\"\n",
    "    Create the Parameterized Quantum Circuit (PQC) for estimating Q-values.\n",
    "    It is the same reported in arXiv:2104.15084 (Skolik et al.).\n",
    "    \n",
    "    Args\n",
    "    -------\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    qr = qk.QuantumRegister(num_qubits, 'qr')\n",
    "    qc = qk.QuantumCircuit(qr)\n",
    "    \n",
    "    if meas:\n",
    "        qr = qk.QuantumRegister(num_qubits, 'qr')\n",
    "        cr = qk.ClassicalRegister(num_qubits, 'cr')\n",
    "        qc = qk.QuantumCircuit(qr,cr)\n",
    "    \n",
    "    \n",
    "    if not reuploading:\n",
    "        \n",
    "        # Define a vector containg Inputs as parameters (*not* to be optimized)\n",
    "        inputs = qk.circuit.ParameterVector('x', num_qubits)\n",
    "        \n",
    "        # Define a weights for the encoding\n",
    "        encoding_weights = qk.circuit.ParameterVector('w', num_qubits)\n",
    "        \n",
    "        # Encode classical input data\n",
    "        qc.compose(encoding_circuit(inputs, encoding_weights, num_qubits = num_qubits), inplace = True)\n",
    "        if insert_barriers: qc.barrier()\n",
    "        \n",
    "        # Variational circuit\n",
    "        qc.compose(TwoLocal(num_qubits, ['ry', 'rz'], 'cx', 'circular', \n",
    "               reps=reps, insert_barriers= insert_barriers, \n",
    "               skip_final_rotation_layer = True), inplace = True)\n",
    "        if insert_barriers: qc.barrier()\n",
    "        \n",
    "        # Add final measurements\n",
    "        if meas: qc.measure(qr,cr)\n",
    "        \n",
    "    elif reuploading:\n",
    "        \n",
    "        # Define a vector containg Inputs as parameters (*not* to be optimized)\n",
    "        inputs = qk.circuit.ParameterVector('x', num_qubits)\n",
    "        \n",
    "        # Define a weights for the encoding\n",
    "        encoding_weights = qk.circuit.ParameterVector('w', num_qubits)\n",
    "        \n",
    "        # Define a vector containng variational parameters\n",
    "        θ = qk.circuit.ParameterVector('θ', 2 * num_qubits * reps)\n",
    "        \n",
    "        # Iterate for a number of repetitions\n",
    "        for rep in range(reps):\n",
    "\n",
    "            # Encode classical input data\n",
    "            qc.compose(encoding_circuit(inputs, encoding_weights, num_qubits = num_qubits), inplace = True)\n",
    "            if insert_barriers: qc.barrier()\n",
    "                \n",
    "            # Variational circuit (does the same as TwoLocal from Qiskit)\n",
    "            for qubit in range(num_qubits):\n",
    "                qc.ry(θ[qubit + 2*num_qubits*(rep)], qubit)\n",
    "                qc.rz(θ[qubit + 2*num_qubits*(rep) + num_qubits], qubit)\n",
    "            if insert_barriers: qc.barrier()\n",
    "                \n",
    "            # Add entanglers (this code is for a circular entangler)\n",
    "            qc.cx(qr[-1], qr[0])\n",
    "            for qubit in range(num_qubits-1):\n",
    "                qc.cx(qr[qubit], qr[qubit+1])\n",
    "            if insert_barriers: qc.barrier()\n",
    "                        \n",
    "        # (Optional) Add final measurements\n",
    "        if meas: qc.measure(qr,cr)\n",
    "        \n",
    "    return qc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import LBFGS, SGD, Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of qubits\n",
    "num_qubits = 4\n",
    "\n",
    "# Generate some random inputs\n",
    "inputs = np.random.rand(num_qubits)\n",
    "inputs = np.random.rand(num_qubits)\n",
    "\n",
    "# Generate the Parametrized Quantum Circuit (note the flag reuploading)\n",
    "qc = parametrized_circuit(num_qubits = num_qubits, reuploading = True, reps = 6)\n",
    "\n",
    "\n",
    "X = list(qc.parameters)[num_qubits: 2*num_qubits]\n",
    "params = list(qc.parameters)[:num_qubits] + list(qc.parameters)[2 * num_qubits:]\n",
    "\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qi_qasm = QuantumInstance(qk.Aer.get_backend('qasm_simulator'))\n",
    "qi = QuantumInstance(qk.Aer.get_backend('statevector_simulator'))\n",
    "\n",
    "qnn = CircuitQNN(qc, input_params=X, weight_params=params, \n",
    "                 quantum_instance = qi)\n",
    "\n",
    "# set up PyTorch module\n",
    "initial_weights = 0.1*(2*np.random.rand(qnn.num_weights) - 1)\n",
    "qnn_model = TorchConnector(qnn, initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class post_processing_layer2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = torch.Tensor(2)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, 1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"description\"\"\"\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        x = x.float()\n",
    "            \n",
    "        mask_ZZ_12 = torch.tensor([1.,-1.,-1.,1.,1.,-1.,-1.,1.,1.,-1.,-1.,1.,1.,-1.,-1.,1.])\n",
    "        expval_ZZ_12 = torch.tensordot(x, mask_ZZ_12, dims = 1).reshape(-1,1)\n",
    "        \n",
    "        mask_ZZ_34 = torch.tensor([-1.,-1.,-1.,-1.,1.,1.,1.,1.,-1.,-1.,-1.,-1.,1.,1.,1.,1.])\n",
    "        expval_ZZ_34 = torch.tensordot(x, mask_ZZ_34, dims = 1).reshape(-1,1)\n",
    "                        \n",
    "        out = torch.cat((expval_ZZ_12, expval_ZZ_34),1)\n",
    "                \n",
    "        return self.weights * ((out + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class post_processing_layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weights = torch.Tensor(2)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, 1, 90)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"description\"\"\"\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "                        \n",
    "        ZZ_12_mask = torch.tensor([0,3,4,7,8,11,12,15]).repeat(x.shape[0], 1)\n",
    "        ZZ_12_mask_not = torch.tensor([1,2,5,6,9,10,13,14]).repeat(x.shape[0], 1)\n",
    "        # \n",
    "        ZZ_34_mask = torch.tensor([0,1,2,3,12,13,14,15]).repeat(x.shape[0], 1)\n",
    "        ZZ_34_mask_not = torch.tensor([4,5,6,7,8,9,10,11]).repeat(x.shape[0], 1)\n",
    "        #         \n",
    "        ZZ12_expval = torch.sum(torch.gather(x, 1, ZZ_12_mask),  dim = 1, keepdims = True) - torch.sum(torch.gather(x, 1, ZZ_12_mask_not),  dim = 1, keepdims = True)\n",
    "        ZZ34_expval = torch.sum(torch.gather(x, 1, ZZ_34_mask),  dim = 1, keepdims = True) - torch.sum(torch.gather(x, 1, ZZ_34_mask_not),  dim = 1, keepdims = True)\n",
    "                \n",
    "        # print(ZZ12_expval)\n",
    "        \n",
    "        out = torch.cat((ZZ12_expval, ZZ34_expval), 1)\n",
    "        \n",
    "        # print(x, x[:,0], self.weights)\n",
    "        \n",
    "        return self.weights * ((x[:,0:1] + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        # print(\"Azione non-random\", state[np.newaxis])\n",
    "        with torch.no_grad():\n",
    "            Q_values = model(torch.atan(Tensor(state))).detach().numpy()\n",
    "        # print(\"Q_values = \", Q_values)\n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_Q_values = model(torch.atan(Tensor(next_states))).detach().numpy()\n",
    "        \n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for j in range(target_Q_values.shape[0]):\n",
    "        q_value = model(torch.atan(Tensor(states[j])))\n",
    "        q_value = q_value[0, actions[j]]\n",
    "        loss += (Tensor(target_Q_values[j]) - q_value).pow(2).sum()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # OLD ROUTINE \n",
    "    #mask = torch.nn.functional.one_hot(Tensor(actions).long(), n_outputs)\n",
    "    ## print(\"qui\")\n",
    "    #\n",
    "    #all_Q_values = model(torch.atan(Tensor(states)))\n",
    "    ## print(\"allQ:\",all_Q_values)\n",
    "    ## print(\"mask:\",mask)\n",
    "    #Q_values = torch.sum(all_Q_values * mask, dim=1, keepdims=True)\n",
    "    ## print(\"taget_q:\",target_Q_values)\n",
    "    ## print(\"qval:\",Q_values)\n",
    "    #loss = loss_fn(Tensor(target_Q_values), Q_values)\n",
    "    #\n",
    "    #optimizer.zero_grad()\n",
    "    #loss.backward()\n",
    "    \n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_model = post_processing_layer2()\n",
    "model = torch.nn.Sequential(qnn_model, classical_model)\n",
    "\n",
    "batch_size = 10\n",
    "discount_rate = 0.99\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = MSELoss(reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "for episode in range(300):\n",
    "    obs = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 200, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step) # Not shown in the book\n",
    "    if step >= best_score: # Not shown\n",
    "        best_weights = model.state_dict # Not shown\n",
    "        best_score = step # Not shown\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "    # print(\"Here!\")\n",
    "    if episode > 50:\n",
    "        # print(\"here\")\n",
    "        training_step(batch_size)\n",
    "\n",
    "#model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.weight = best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "#save_fig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(action, state, done)\n",
    "    if done:\n",
    "        print(\"End at step:\", step)\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
